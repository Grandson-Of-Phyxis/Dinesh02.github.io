<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Divyansh Jhunjhunwala</title>
  
  <meta name="author" content="Divyansh Jhunjhunwala">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:130%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Divyansh Jhunjhunwala</name>
              </p>

              <heading>About Me</heading>

		    
              <p> Hi! I am Divyansh, a fourth year PhD candidate in the <a href="https://www.ece.cmu.edu/">Electrical and Computer Engineering</a> department at <a href = "https://www.cmu.edu/">Carnegie Mellon University</a>, advised by <a href="https://www.andrew.cmu.edu/user/gaurij/">Dr. Gauri Joshi</a>. 
                My research interests lie broadly in distributed optimization and machine learning, in particular federated learning. Given the multi-disciplinary nature of problems in federated learning, my research often leverages ideas from related areas such as optimization theory, transfer learning, model fusion, theory of over-parameterized neural networks, among others.
              </p>
              <p>
                In summer 22 and summer 23, I interned at IBM Research working with <a href="https://shiqiang.wang/">Dr. Shiqiang Wang</a> on some interesting problems in federated learning.
              </p>

              <p>

                Prior to CMU, I completed my Bachelors in Technology (B.Tech) in <a href="http://www.ecdept.iitkgp.ac.in/">Electronics and Electrical Communication Engineering</a> from <a href = "http://www.iitkgp.ac.in/">IIT Kharagpur</a>, where I
                received the Institute Silver Medal for graduating with the highest CGPA in my department.
              </p>
              
              <p style="text-align:center">
                <a href="mailto:djhunjhu@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=0E54wbUAAAAJ&hl=en&oi=ao">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:70%;max-width:70%">
              <a href="data/divyansh_website_photo.png"><img style="width:70%;max-width:70%" alt="profile photo" src="data/divyansh_website_photo-modified.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:130%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent News</heading>
	      <p>
                <strong> Jan 24: </strong> My work on <a href = "http://arxiv.org/abs/2403.12329"> one-shot federated learning using Fisher information </a> got accepted to AISTATS 2024!
              </p>
	      <p>
                <strong> Sep 23: </strong> Attended the New Frontiers in Federated Learning Workshop at Toyota Institute of Chicago (TTIC). Thanks to all the organizers!
              </p>
	      <p>
                <strong> May 23: </strong> I am returning to IBM Research, Yorktown Heights as a summer research intern.
              </p>
	      <p>
                <strong> Jan 23: </strong> My internship work on <a href = https://arxiv.org/abs/2301.09604> tuning the server step size in federated learning </a> was accepted as a spotlight presentation at ICLR 2023!
              </p>
              <p>
                <strong> Oct 22: </strong> Our work on <a href = "https://arxiv.org/abs/2205.14840"> incentivizing clients for federated learning </a> was accepted as an oral presentation at the <a href = "https://federated-learning.org/fl-neurips-2022/ "> FL-Neurips 22 workshop!</a> (12% acceptance rate).
              </p>
              <p>
                <strong> Aug 22: </strong> Completed my internship at <a href="https://research.ibm.com/labs/watson/"> IBM T.J. Watson Research Center, New York</a>.
              </p>
              <p>
                <strong> April 22: </strong> Our team was selected as a finalist for the <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-north-america"> Qualcomm Innovation Fellowship</a> for the research proposal 
		      "Incentivized Federated Learning for Data-Heterogeneous and Resource-Constrained Clients".
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:130%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:130%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

		<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/few_shot_cifar.jpg" alt="few shot image" width="210" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                <a href = "http://arxiv.org/abs/2403.12329"><papertitle>FedFisher: Leveraging Fisher Information for One-Shot Federated Learning </papertitle> </a>
                  <br>
			<strong> Divyansh Jhunjhunwala </strong>, Shiqiang Wang, Gauri Joshi
                  <br>
                  <em> International Conference on Artificial Intelligence and Statistics (AISTATS) 2024 </em>
		  <br>
		  <em> A preliminary version appeared at Federated Learning and Analytics workshop at ICML 2023 </em>
                  <br>
                  <p> Propose FedFisher, an algorithm for learning the global model for federated learning using just one round communication with novel theotetical guarantees for two layer overparameterized ReLU networks.</p>
                </td>
              </tr>

            
		<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/matrix_test_acc.jpg" alt="inc fl image" width="210" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                <a href = "https://arxiv.org/abs/2301.09604"><papertitle>FedExP: Speeding up Federated Averaging via Extrapolation </papertitle> </a>
                  <br>
			<strong> Divyansh Jhunjhunwala </strong>, Shiqiang Wang, Gauri Joshi
                  <br>
                  <em>International Conference on Learning Representations (ICLR), 2023 (<strong> Spotlight, top 25% of accepted papers </strong>)</em>
                  <br>
                  <p>We present FedExP, a method to adaptively determine the server step size in FL based on dynamically varying pseudo-gradients throughout the FL process.</p>
                </td>
              </tr>
		
		
		<tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/inc_fl.jpg" alt="inc fl image" width="250" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                <a href = "https://openreview.net/pdf?id=8GI1SXqJBk"><papertitle>Maximizing Global Model Appeal in Federated Learning</papertitle> </a>
                  <br>
                  Yae Jee Cho,  <strong> Divyansh Jhunjhunwala </strong>, Tian Li, Virginia Smith, Gauri Joshi
                  <br>
                  <em>Transactions of Machine Learning Research (TMLR), 2024 </em>
                  <br>
                  <p>Propose MaxFL algorithm to explicitly maximize the fraction of clients that are incentivized to use the global model in federated learning.</p>
                </td>
              </tr>

            
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fedvarp.jpg" alt="fedvarp image" width="230" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                <a href = "https://arxiv.org/abs/2207.14130"><papertitle>FedVARP: Tackling the Variance Due to Partial Client Participation in Federated Learning</papertitle></a>
                  <br>
                  <strong> Divyansh Jhunjhunwala </strong>, Pranay Sharma, Aushim Nagarkatti, Gauri Joshi
                  <br>
                  <em> Uncertainty in Artificial Intelligence (UAI), 2022</em>
                  <br>
                  <p>Propose FedVARP algorithm to deal with variance caused by only a few clients participating in every round of federated training.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/spatial_temporal.jpg" alt="spatial image" width="250" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2110.07751"><papertitle> Leveraging Spatial and Temporal Correlations in Sparsified Mean Estimation </papertitle></a>
                  <br>
                  <strong> Divyansh Jhunjhunwala </strong>, Ankur Mallick, Advait Gadhikar, Swanand Kadhe, Gauri Joshi
                  <br>
                  <em> Advances in Neural Information Processing Systems (NeurIPS), 2021</em>
                  <br>
                  <p> Introduce notions of spatial and temporal correlations and show how they can be used to efficiently compute the mean of a set of vectors in a communication-limited setting. </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/adaquant_fl.jpg" alt="adaquant fl image" width="250" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                <a href = "https://arxiv.org/abs/2102.04487"><papertitle> Adaptive Quantization of model updates for communication-efficient federated learning  </papertitle></a>
                  <br>
                  <strong> Divyansh Jhunjhunwala </strong>, Advait Gadhikar, Gauri Joshi, Yonina C. Eldar
                  <br>
                  <em> International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021</em>
                  <br>
                  <p> Propose an adaptive quantization strategy that aims to achieve communication efficiency as well as a low error floor by changing the number of quantization levels during training in federated learning.</p>
                </td>
              </tr>

        </tbody></table>





        <table style="width:120%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    Source code credit to <a href ="https://github.com/jonbarron/website "> Dr. Jon Barron. </a>
                </p>
              </td>
            </tr>
        </tbody></table>

            
      </td>
    </tr>
  </table>
</body>

</html>
